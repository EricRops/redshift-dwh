import configparser

# CONFIG
config = configparser.ConfigParser()
config.read('dwh.cfg')

# DROP TABLES

staging_events_table_drop = "DROP TABLE IF EXISTS staging_events;"
staging_songs_table_drop = "DROP TABLE IF EXISTS staging_songs;"
songplay_table_drop = "DROP TABLE IF EXISTS songplays;"
user_table_drop = "DROP TABLE IF EXISTS users;"
song_table_drop = "DROP TABLE IF EXISTS songs;"
artist_table_drop = "DROP TABLE IF EXISTS artists;"
time_table_drop = "DROP TABLE IF EXISTS time;"

# CREATE TABLES

# All columns from the events data
# NOTE: the start_time column has format like "1541106106796." We will copy
# it into the staging table as BIGINT, and then convert it to a proper timestamp
# when bringing it into the analytics table
staging_events_table_create = ("""CREATE TABLE IF NOT EXISTS staging_events \
                                                               (artist VARCHAR, \
                                                                auth VARCHAR, \
                                                                first_name VARCHAR, \
                                                                gender VARCHAR, \
                                                                item_in_session INTEGER, \
                                                                last_name VARCHAR, \
                                                                duration REAL, \
                                                                level VARCHAR, \
                                                                location VARCHAR, \
                                                                method VARCHAR, \
                                                                page VARCHAR, \
                                                                registration REAL, \
                                                                session_id INTEGER, \
                                                                song VARCHAR, \
                                                                status INTEGER, \
                                                                start_time BIGINT, \
                                                                user_agent VARCHAR, \
                                                                user_id INTEGER);""")

# All columns from the song data
staging_songs_table_create = ("""CREATE TABLE IF NOT EXISTS staging_songs \
                                                           (num_songs INTEGER, \
                                                            artist_id VARCHAR, \
                                                            artist_latitude REAL, \
                                                            artist_longitude REAL, \
                                                            artist_location VARCHAR, \
                                                            artist_name VARCHAR, \
                                                            song_id VARCHAR, \
                                                            title VARCHAR, \
                                                            duration REAL, \
                                                            year INTEGER);""")

# songplay_id as IDENTITY(seed=0, step=1) (ids are autogenerated by computer, same as SERIAL from Postgres)
# UPDATE: NO LONGER using IDENTITY key as it makes duplicate row detection very difficult
songplay_table_create = ("""CREATE TABLE IF NOT EXISTS songplays (songplay_id INTEGER NOT NULL PRIMARY KEY, \
                                                                start_time TIMESTAMP NOT NULL sortkey, \
                                                                user_id INTEGER NOT NULL, \
                                                                level VARCHAR, \
                                                                song_id VARCHAR, \
                                                                artist_id VARCHAR, \
                                                                session_id INTEGER NOT NULL, \
                                                                location VARCHAR, \
                                                                user_agent VARCHAR);""")

user_table_create = ("""CREATE TABLE IF NOT EXISTS users (user_id INTEGER NOT NULL PRIMARY KEY sortkey, \
                                                        first_name VARCHAR, \
                                                        last_name VARCHAR, \
                                                        gender VARCHAR, \
                                                        level VARCHAR);""")

song_table_create = ("""CREATE TABLE IF NOT EXISTS songs (song_id VARCHAR NOT NULL PRIMARY KEY, \
                                                        title VARCHAR sortkey, \
                                                        artist_id VARCHAR, \
                                                        year INTEGER, \
                                                        duration REAL);""")

artist_table_create = ("""CREATE TABLE IF NOT EXISTS artists (artist_id VARCHAR NOT NULL PRIMARY KEY, \
                                                            name VARCHAR sortkey, \
                                                            location VARCHAR, \
                                                            latitude REAL, \
                                                            longitude REAL);""")

time_table_create = ("""CREATE TABLE IF NOT EXISTS time (start_time TIMESTAMP NOT NULL PRIMARY KEY sortkey, \
                                                       hour INTEGER, \
                                                       day INTEGER, \
                                                       week INTEGER, \
                                                       month INTEGER, \
                                                       year INTEGER, \
                                                       weekday INTEGER);""")

# STAGING TABLES

# Can use the 'auto' format here because the JSON keys match the column names EXACTLY.
# If the column names don't match it will not work
staging_songs_copy = ("""copy staging_songs from {} 
                         iam_role '{}'
                         json 'auto';
                      """).format(config.get("S3", "SONG_DATA"), config.get("IAM_ROLE", "ARN"))

# For the staging_events table, the JSON key labels DO NOT MATCH the final column names we want.
# We need to use a JSON paths file to map the order of the JSON keys to the final table.
staging_events_copy = ("""copy staging_events from {} 
                          iam_role '{}'
                          json {};
                       """).format(config.get("S3", "LOG_DATA"), config.get("IAM_ROLE", "ARN"), \
                                   config.get("S3", "LOG_JSONPATH"))

# FINAL TABLES

# Fact Table (songplays):
# NOTE: using ROW_NUMBER to populate the songplay_id column
# If we used IDENTITY and songplay_id was autofilled, it would be difficult to check for duplicate rows.
# Filter to NextSong events only
songplay_table_insert = ("""
    INSERT INTO songplays (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
    SELECT
        ROW_NUMBER() OVER (PARTITION BY 1),
        TIMESTAMP 'epoch' + se.start_time::numeric/1000 *INTERVAL '1 second',
        se.user_id,
        se.level,
        ss.song_id,
        ss.artist_id,
        se.session_id,
        se.location,
        se.user_agent
    FROM staging_events se
    LEFT JOIN staging_songs ss ON (se.artist = ss.artist_name) AND (se.song = ss.title)
    WHERE se.page = 'NextSong';
""")

# For Dimension tables:
# Including DISTINCT (primary_key) to prevent duplicate rows getting inserted
# because Redshift does NOT enforce the Primary Key constaint

# For users table, filter the staging table to the most recent start_time,
# to capture the most recent "level" status.
# NOTE: Do not use WHERE page="NextSong" because the most recent page for 
# several users is "Logout," so we would otherwise drop all those users
user_table_insert = ("""
    INSERT INTO users (user_id, first_name, last_name, gender, level)
    SELECT
        DISTINCT s1.user_id,
        s1.first_name,
        s1.last_name,
        s1.gender,
        s1.level
    FROM staging_events s1
    WHERE s1.user_id IS NOT NULL
    AND s1.start_time = (
        SELECT MAX(start_time) 
        FROM staging_events s2
        WHERE s1.user_id = s2.user_id
        )
    ORDER BY s1.user_id
""")

song_table_insert = ("""
    INSERT INTO songs (song_id, title, artist_id, year, duration)
    SELECT
        DISTINCT song_id,
        title,
        artist_id,
        year,
        duration
    FROM staging_songs
    ORDER BY title
""")

artist_table_insert = ("""
    INSERT INTO artists (artist_id, name, location, latitude, longitude)
    SELECT
        DISTINCT artist_id,
        artist_name,
        artist_location,
        artist_latitude,
        artist_longitude
    FROM staging_songs
    ORDER BY artist_name
""")

time_table_insert = ("""
    INSERT INTO time (start_time, hour, day, week, month, year, weekday)
    SELECT
        DISTINCT TIMESTAMP 'epoch' + start_time::numeric/1000 *INTERVAL '1 second' AS start_time2,
        DATE_PART(hr, start_time2),
        DATE_PART(d, start_time2),
        DATE_PART(w, start_time2),
        DATE_PART(mon, start_time2),
        DATE_PART(yr, start_time2),
        DATE_PART(dow, start_time2)
    FROM staging_events
    WHERE page = 'NextSong'
    ORDER BY start_time
""")

# QUERY LISTS

create_table_queries = [staging_events_table_create, staging_songs_table_create, songplay_table_create,
                        user_table_create, song_table_create, artist_table_create, time_table_create]
drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, user_table_drop,
                      song_table_drop, artist_table_drop, time_table_drop]
copy_table_queries = [staging_events_copy, staging_songs_copy]
insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert,
                        time_table_insert]

# DATA QUALITY CHECKS

# Method to remove duplicates taken from:
# medium.com/@elliotchance/removing-duplicate-data-in-redshift-45a43b7ae334
# table and pkey variables specified in etl.py

# This should work on all tables including songplays 
# now that we've removed the identity column
remove_duplicate_rows = ("""
    BEGIN;
    
    -- First identify all the rows that are duplicate
    CREATE TEMP TABLE duplicate_ids AS
    SELECT {pkey}
    FROM {table}
    GROUP BY {pkey}
    HAVING COUNT(*) > 1;
    
    -- Extract one copy of all the duplicate rows
    CREATE TEMP TABLE new_table(LIKE {table});
    
    INSERT INTO new_table
    SELECT DISTINCT *
    FROM {table}
    WHERE {pkey} IN(
         SELECT {pkey}
         FROM duplicate_ids
    );
    
    -- Remove all rows that were duplicated (all copies).
    DELETE FROM {table}
    WHERE {pkey} IN(
         SELECT {pkey}
         FROM duplicate_ids
    );
    
    -- Insert back in the single copies
    INSERT INTO {table}
    SELECT *
    FROM new_table;
    
    -- Cleanup
    DROP TABLE duplicate_ids;
    DROP TABLE new_table;
    
    COMMIT;
""")
